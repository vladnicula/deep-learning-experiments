{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# looking at https://www.youtube.com/watch?v=u4alGiomYP4&t=1832s\n",
    "# this is the second version, adding deep neuron layers to the inital solution\n",
    "# which achieves 97% accuracy on test: \n",
    "# http://localhost:8888/notebooks/projects\n",
    "# /deep-learning-nano/neural-net-tensorflow/digit-recognition-improvement-02.ipynb\n",
    "\n",
    "\n",
    "# Will attempt to implement the extra features:\n",
    "# 1. Update the code to properly use softmax on the last layer, looking at examples\n",
    "# from the github repo\n",
    "# \n",
    "# 2. Use a dynamic learning rate, this means a new input parameter that will configure\n",
    "# the gradient optimizer, starting from 0.003 and reaching 0.0001\n",
    "\n",
    "# resulting accuracy over 98.4%!! :D  \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# we need math for learning rate calculation\n",
    "import math\n",
    "\n",
    "# one_hot = True means we represent the data in a vector way, see below\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "\n",
    "# random seeding 0 means we will get the same random numbers each time we run the program\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use the truncated_normal as it is the recomended random to use for the relu \n",
    "# activation function\n",
    "X = tf.placeholder(tf.float32, [None, 784], 'image')\n",
    "\n",
    "\n",
    "# first layer - relu activation\n",
    "W1 = tf.Variable(tf.truncated_normal([784,200], stddev=0.1), 'l1_weights')\n",
    "B1 = tf.Variable(tf.truncated_normal([200], stddev=0.1), 'l1_baiases')\n",
    "Y1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "\n",
    "\n",
    "# second - relu activation\n",
    "W2 = tf.Variable(tf.truncated_normal([200,100], stddev=0.1), 'l2_weights')\n",
    "B2 = tf.Variable(tf.truncated_normal([100], stddev=0.1), 'l2_baiases')\n",
    "Y2 = tf.nn.relu(tf.add(tf.matmul(Y1, W2), B2))\n",
    "\n",
    "\n",
    "# hird - relu activation\n",
    "W3 = tf.Variable(tf.truncated_normal([100,60], stddev=0.1), 'l3_weights')\n",
    "B3 = tf.Variable(tf.truncated_normal([60], stddev=0.1), 'l3_baiases')\n",
    "Y3 = tf.nn.relu(tf.add(tf.matmul(Y2, W3), B3))\n",
    "\n",
    "\n",
    "# forth - relu activation\n",
    "W4 = tf.Variable(tf.truncated_normal([60,30], stddev=0.1), 'l4_weights')\n",
    "B4 = tf.Variable(tf.truncated_normal([30], stddev=0.1), 'l4_baiases')\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3, W4) + B4)\n",
    "\n",
    "# last (output one) - softmax activation\n",
    "W5 = tf.Variable(tf.truncated_normal([30,10], stddev=0.1), 'output_weights')\n",
    "B5 = tf.Variable(tf.truncated_normal([10], stddev=0.1), 'output_baiases')\n",
    "# this was the mistake I made in the previous improvement.\n",
    "# TODO think of why I need to feed YLogits in the cost functiojn\n",
    "Ylogits = tf.matmul(Y4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "\n",
    "# we delcare the expected correct answers placeholder which will be used to provide the expected results\n",
    "# for each set of images. This will be used in the flow to calculate the error value\n",
    "Y_ = tf.placeholder(tf.float32, shape=[None, 10], name='expected_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cost = tf.reduce_mean(cost)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learning rate input\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "max_learning_rate = 0.003\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 20.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "\n",
    "# learning rate decay\n",
    "def get_learning_rate(i):\n",
    "    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimizer taking the learning rate input which will\n",
    "# be configured at each step and will minimize the corss entropy function\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as first version:\n",
    "\n",
    "# % of correct answers found in batch\n",
    "# based on the Y and Y_ which will be geneated during the runtime of the tests\n",
    "# we compose this is_correct to compute how right we are during training\n",
    "is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 0, 'learning rate', 0.003)\n",
      "('Current training accuracy', 0.94999999, ' current error', 15.603757)\n",
      "('Current testing accuracy', 0.96200001)\n",
      "('Epoch', 1, 'learning rate', 0.0028585653310520707)\n",
      "('Current training accuracy', 0.99000001, ' current error', 8.8755608)\n",
      "('Current testing accuracy', 0.96990001)\n",
      "('Epoch', 2, 'learning rate', 0.0027240285123042826)\n",
      "('Current training accuracy', 0.98000002, ' current error', 5.0459113)\n",
      "('Current testing accuracy', 0.96829998)\n",
      "('Epoch', 3, 'learning rate', 0.0025960531316326675)\n",
      "('Current training accuracy', 1.0, ' current error', 2.0431979)\n",
      "('Current testing accuracy', 0.97570002)\n",
      "('Epoch', 4, 'learning rate', 0.0024743191839261473)\n",
      "('Current training accuracy', 0.99000001, ' current error', 2.8697402)\n",
      "('Current testing accuracy', 0.97420001)\n",
      "('Epoch', 5, 'learning rate', 0.002358522270907074)\n",
      "('Current training accuracy', 1.0, ' current error', 1.0245142)\n",
      "('Current testing accuracy', 0.9745)\n",
      "('Epoch', 6, 'learning rate', 0.002248372839976982)\n",
      "('Current training accuracy', 1.0, ' current error', 0.18142207)\n",
      "('Current testing accuracy', 0.97820002)\n",
      "('Epoch', 7, 'learning rate', 0.002143595460184269)\n",
      "('Current training accuracy', 1.0, ' current error', 0.75601995)\n",
      "('Current testing accuracy', 0.97640002)\n",
      "('Epoch', 8, 'learning rate', 0.002043928133503354)\n",
      "('Current training accuracy', 0.99000001, ' current error', 1.2983007)\n",
      "('Current testing accuracy', 0.97570002)\n",
      "('Epoch', 9, 'learning rate', 0.001949121639703143)\n",
      "('Current training accuracy', 1.0, ' current error', 0.38646111)\n",
      "('Current testing accuracy', 0.97860003)\n",
      "('Epoch', 10, 'learning rate', 0.0018589389131666372)\n",
      "('Current training accuracy', 1.0, ' current error', 0.16740468)\n",
      "('Current testing accuracy', 0.97710001)\n",
      "('Epoch', 11, 'learning rate', 0.0017731544501034114)\n",
      "('Current training accuracy', 0.99000001, ' current error', 6.9004374)\n",
      "('Current testing accuracy', 0.97600001)\n",
      "('Epoch', 12, 'learning rate', 0.0016915537446726768)\n",
      "('Current training accuracy', 0.99000001, ' current error', 1.2644248)\n",
      "('Current testing accuracy', 0.97799999)\n",
      "('Epoch', 13, 'learning rate', 0.0016139327526069466)\n",
      "('Current training accuracy', 1.0, ' current error', 0.22504677)\n",
      "('Current testing accuracy', 0.97860003)\n",
      "('Epoch', 14, 'learning rate', 0.0015400973809950877)\n",
      "('Current training accuracy', 1.0, ' current error', 0.0062152501)\n",
      "('Current testing accuracy', 0.97939998)\n",
      "('Epoch', 15, 'learning rate', 0.0014698630029489428)\n",
      "('Current training accuracy', 1.0, ' current error', 0.012238357)\n",
      "('Current testing accuracy', 0.98110002)\n",
      "('Epoch', 16, 'learning rate', 0.0014030539959399427)\n",
      "('Current training accuracy', 1.0, ' current error', 0.037381701)\n",
      "('Current testing accuracy', 0.97839999)\n",
      "('Epoch', 17, 'learning rate', 0.0013395033026513076)\n",
      "('Current training accuracy', 1.0, ' current error', 0.039365463)\n",
      "('Current testing accuracy', 0.97820002)\n",
      "('Epoch', 18, 'learning rate', 0.0012790520132477375)\n",
      "('Current training accuracy', 1.0, ' current error', 0.12249715)\n",
      "('Current testing accuracy', 0.9788)\n",
      "('Epoch', 19, 'learning rate', 0.0012215489680180538)\n",
      "('Current training accuracy', 1.0, ' current error', 0.13850901)\n",
      "('Current testing accuracy', 0.98049998)\n",
      "('Epoch', 20, 'learning rate', 0.0011668503793971828)\n",
      "('Current training accuracy', 1.0, ' current error', 0.040559899)\n",
      "('Current testing accuracy', 0.98049998)\n",
      "('Epoch', 21, 'learning rate', 0.0011148194724223506)\n",
      "('Current training accuracy', 1.0, ' current error', 0.016920175)\n",
      "('Current testing accuracy', 0.97909999)\n",
      "('Epoch', 22, 'learning rate', 0.0010653261427244307)\n",
      "('Current training accuracy', 1.0, ' current error', 0.01477786)\n",
      "('Current testing accuracy', 0.98180002)\n",
      "('Epoch', 23, 'learning rate', 0.0010182466311992545)\n",
      "('Current training accuracy', 1.0, ' current error', 0.00065577496)\n",
      "('Current testing accuracy', 0.98250002)\n",
      "('Epoch', 24, 'learning rate', 0.0009734632145453863)\n",
      "('Current training accuracy', 1.0, ' current error', 0.093025215)\n",
      "('Current testing accuracy', 0.98110002)\n",
      "('Epoch', 25, 'learning rate', 0.0009308639108945514)\n",
      "('Current training accuracy', 1.0, ' current error', 0.12952417)\n",
      "('Current testing accuracy', 0.97909999)\n",
      "('Epoch', 26, 'learning rate', 0.0008903421997986366)\n",
      "('Current training accuracy', 1.0, ' current error', 0.11732779)\n",
      "('Current testing accuracy', 0.97829998)\n",
      "('Epoch', 27, 'learning rate', 0.0008517967558730855)\n",
      "('Current training accuracy', 1.0, ' current error', 0.0013019574)\n",
      "('Current testing accuracy', 0.98280001)\n",
      "('Epoch', 28, 'learning rate', 0.0008151311954306589)\n",
      "('Current training accuracy', 1.0, ' current error', 0.00050349301)\n",
      "('Current testing accuracy', 0.98269999)\n",
      "('Epoch', 29, 'learning rate', 0.0007802538354720133)\n",
      "('Current training accuracy', 1.0, ' current error', 0.0022583271)\n",
      "('Current testing accuracy', 0.98400003)\n",
      "('Final training accuracy', 1.0)\n",
      "('Final testing accuracy', 0.98400003)\n"
     ]
    }
   ],
   "source": [
    "# we start our trainig session\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for _ in range(30):\n",
    "        \n",
    "        learning_rate = get_learning_rate(_)\n",
    "        \n",
    "        print('Epoch', _, 'learning rate', learning_rate)\n",
    "        \n",
    "        for i in range(int(mnist.train.num_examples/100)):\n",
    "            # load a btach of images, thanks to mnist\n",
    "            batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "            \n",
    "            train_data = {X:batch_X, Y_:batch_Y, lr: learning_rate}\n",
    "            \n",
    "            # train the network one time\n",
    "            sess.run(train_step, feed_dict=train_data)\n",
    "\n",
    "\n",
    "        # get the accuracy after training\n",
    "        a, c = sess.run([accuracy, cost], feed_dict=train_data)\n",
    "        print('Current training accuracy', a, ' current error', c)\n",
    "        print('Current testing accuracy', accuracy.eval({X:mnist.test.images, Y_:mnist.test.labels}))\n",
    "\n",
    "    a, c = sess.run([accuracy, cost], feed_dict=train_data)\n",
    "    print('Final training accuracy', a)        \n",
    "    print('Final testing accuracy', accuracy.eval({X:mnist.test.images, Y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
