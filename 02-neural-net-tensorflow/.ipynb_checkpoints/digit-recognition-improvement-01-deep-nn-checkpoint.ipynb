{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# looking at https://www.youtube.com/watch?v=u4alGiomYP4&t=1832s\n",
    "# this is the second version, adding deep neuron layers to the inital solution\n",
    "# which achieves 92% accuracy on test: \n",
    "# http://localhost:8888/notebooks/projects\n",
    "# /deep-learning-nano/neural-net-tensorflow/digit-recognition-initial.ipynb\n",
    "\n",
    "\n",
    "# Will attempt to implement the following algorithm\n",
    "# 1. Have 4-5 hidden layers, starting form a higher number of neurons and going to a lower number\n",
    "# \n",
    "# 2. Will use relu for outputing the values from one layer to the other as it seesm to be \n",
    "# the closest representation we have now as to how biological neurons do it. Sigmoid would also be\n",
    "# an option, but according to smart google guy, relu is better. \n",
    "#\n",
    "# 3. Use softmax for the output layer to force the values to be between 0 and 1. \n",
    "#\n",
    "# - Will still expect the response to be in the one_hot representation [1, 0, ....] for 0, \n",
    "#  [0, 1, 0, ....] for 1 etc\n",
    "# - Will still feed batches of 100 images, each one represented as one line of integers\n",
    "# - Will still do a matrix multiplication of InputMatrix(100, 28*28) with FirstLayer(28*28, New_Neuron_Count)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# one_hot = True means we represent the data in a vector way, see below\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we declare a placeholder variable which will receive None (1-100 something) images of 28 by 28 with one value\n",
    "# in each column\n",
    "X = tf.placeholder(tf.float32, [None, 784], 'image')\n",
    "\n",
    "\n",
    "# first layer - relu activation\n",
    "W1 = tf.Variable(tf.random_normal([784,500]), 'l1_weights')\n",
    "B1 = tf.Variable(tf.random_normal([500]), 'l1_baiases')\n",
    "Y1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "\n",
    "\n",
    "# second - relu activation\n",
    "W2 = tf.Variable(tf.random_normal([500,620]), 'l2_weights')\n",
    "B2 = tf.Variable(tf.random_normal([620]), 'l2_baiases')\n",
    "Y2 = tf.nn.relu(tf.add(tf.matmul(Y1, W2), B2))\n",
    "\n",
    "\n",
    "# hird - relu activation\n",
    "W3 = tf.Variable(tf.random_normal([620,300]), 'l3_weights')\n",
    "B3 = tf.Variable(tf.random_normal([300]), 'l3_baiases')\n",
    "Y3 = tf.nn.relu(tf.add(tf.matmul(Y2, W3), B3))\n",
    "\n",
    "\n",
    "# forth - relu activation\n",
    "#W4 = tf.Variable(tf.zeros([500,100]), 'l4_weights')\n",
    "#B4 = tf.Variable(tf.zeros([100]), 'l4_baiases')\n",
    "#Y4 = tf.nn.relu(tf.matmul(Y3, W4) + B4)\n",
    "\n",
    "# last (output one) - softmax activation\n",
    "W5 = tf.Variable(tf.random_normal([300,10]), 'output_weights')\n",
    "B5 = tf.Variable(tf.random_normal([10]), 'output_baiases')\n",
    "#Y = tf.nn.softmax(tf.matmul(Y4, W5) + B5)\n",
    "Y = tf.matmul(Y3, W5) + B5\n",
    "\n",
    "\n",
    "# we delcare the expected correct answers placeholder which will be used to provide the expected results\n",
    "# for each set of images. This will be used in the flow to calculate the error value\n",
    "Y_ = tf.placeholder(tf.float32, shape=[None, 10], name='expected_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as first version: \n",
    "\n",
    "# We declare the loss function\n",
    "# this function will calculate a value that tells us how bad we scrued up. How different\n",
    "# is the value we got from the netwrok with the value we wanted. It is a function based\n",
    "# on the expected result and the received result. \n",
    "# \n",
    "# Note: now it is just a declaration. A component that will serve this purpose when we hook things\n",
    "# up in the tensurflow flow\n",
    "#cross_entropy const\n",
    "#cost = -tf.reduce_sum(Y_ * tf.log(Y))\n",
    "\n",
    "# the other cost form video tutorial\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=Y_) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as fisrt version:\n",
    "\n",
    "# we declare the optimizer, a gradient descent which will take the\n",
    "# declared corss_entropy function and based on the values fromm there\n",
    "# will go down, trying to reduce the value of that function by changing the weights.\n",
    "# this declaration will be used by tensorflow to change the weights from the\n",
    "# W and b matrices\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.005)\n",
    "#train_step = optimizer.minimize(cost)\n",
    "\n",
    "# learning_rate = 0.001 is the default learning rate for this optimizer\n",
    "train_step = tf.train.AdamOptimizer(0.002).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as first version:\n",
    "\n",
    "# % of correct answers found in batch\n",
    "# based on the Y and Y_ which will be geneated during the runtime of the tests\n",
    "# we compose this is_correct to compute how right we are during training\n",
    "is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "('Current training accuracy', 0.93000001, ' current error', 692.30127)\n",
      "('Current testing accuracy', 0.89960003)\n",
      "1\n",
      "('Current training accuracy', 0.89999998, ' current error', 783.89362)\n",
      "('Current testing accuracy', 0.92049998)\n",
      "2\n",
      "('Current training accuracy', 0.95999998, ' current error', 369.79114)\n",
      "('Current testing accuracy', 0.92659998)\n",
      "3\n",
      "('Current training accuracy', 0.95999998, ' current error', 157.68361)\n",
      "('Current testing accuracy', 0.93470001)\n",
      "4\n",
      "('Current training accuracy', 0.97000003, ' current error', 89.65332)\n",
      "('Current testing accuracy', 0.93940002)\n",
      "5\n",
      "('Current training accuracy', 0.98000002, ' current error', 55.4921)\n",
      "('Current testing accuracy', 0.93529999)\n",
      "6\n",
      "('Current training accuracy', 0.99000001, ' current error', 1.5344825)\n",
      "('Current testing accuracy', 0.94279999)\n",
      "7\n",
      "('Current training accuracy', 1.0, ' current error', 0.0)\n",
      "('Current testing accuracy', 0.9479)\n",
      "8\n",
      "('Current training accuracy', 1.0, ' current error', 0.0)\n",
      "('Current testing accuracy', 0.94459999)\n",
      "9\n",
      "('Current training accuracy', 1.0, ' current error', 0.0)\n",
      "('Current testing accuracy', 0.94679999)\n",
      "10\n",
      "('Current training accuracy', 1.0, ' current error', 0.0)\n",
      "('Current testing accuracy', 0.95090002)\n",
      "11\n",
      "('Current training accuracy', 0.99000001, ' current error', 0.49319336)\n",
      "('Current testing accuracy', 0.95450002)\n",
      "12\n",
      "('Current training accuracy', 0.99000001, ' current error', 1.7233008)\n",
      "('Current testing accuracy', 0.95539999)\n",
      "13\n",
      "('Current training accuracy', 1.0, ' current error', 0.0)\n",
      "('Current testing accuracy', 0.95370001)\n",
      "14\n",
      "('Current training accuracy', 1.0, ' current error', 0.0)\n",
      "('Current testing accuracy', 0.95560002)\n",
      "15\n",
      "('Current training accuracy', 0.99000001, ' current error', 29.065392)\n",
      "('Current testing accuracy', 0.9558)\n",
      "16\n",
      "('Current training accuracy', 0.99000001, ' current error', 2.9576757)\n",
      "('Current testing accuracy', 0.95639998)\n",
      "17\n",
      "('Current training accuracy', 1.0, ' current error', 0.0)\n",
      "('Current testing accuracy', 0.96060002)\n",
      "18\n",
      "('Current training accuracy', 1.0, ' current error', 0.0)\n",
      "('Current testing accuracy', 0.96079999)\n",
      "19\n",
      "('Current training accuracy', 1.0, ' current error', 0.0)\n",
      "('Current testing accuracy', 0.96259999)\n",
      "('Final training accuracy', 1.0)\n",
      "('Final testing accuracy', 0.96259999)\n"
     ]
    }
   ],
   "source": [
    "# we start our trainig session\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for _ in range(30):\n",
    "        \n",
    "        print _\n",
    "        \n",
    "        for i in range(int(mnist.train.num_examples/100)):\n",
    "            # load a btach of images, thanks to mnist\n",
    "            batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "            train_data = {X:batch_X, Y_:batch_Y}\n",
    "\n",
    "            # train the network one time\n",
    "            sess.run(train_step, feed_dict=train_data)\n",
    "\n",
    "\n",
    "        # get the accuracy after training\n",
    "        a, c = sess.run([accuracy, cost], feed_dict=train_data)\n",
    "        print('Current training accuracy', a, ' current error', c)\n",
    "        print('Current testing accuracy', accuracy.eval({X:mnist.test.images, Y_:mnist.test.labels}))\n",
    "\n",
    "    a, c = sess.run([accuracy, cost], feed_dict=train_data)\n",
    "    print('Final training accuracy', a)        \n",
    "    print('Final testing accuracy', accuracy.eval({X:mnist.test.images, Y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
