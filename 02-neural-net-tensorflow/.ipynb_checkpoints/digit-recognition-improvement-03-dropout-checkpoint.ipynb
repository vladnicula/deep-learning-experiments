{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# looking at https://www.youtube.com/watch?v=u4alGiomYP4&t=1832s\n",
    "# this is the third version, adding dropout via tensorflow implementation\n",
    "# which achieves 98.4% accuracy on previous step \n",
    "\n",
    "# in this 'improvement' we get 97% only, but this will allow us to learn\n",
    "# more complex problems. If we care about rotation, scale and the stuch\n",
    "# then this mgiht be a helpful addition\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# we need math for learning rate calculation\n",
    "import math\n",
    "\n",
    "# one_hot = True means we represent the data in a vector way, see below\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "\n",
    "# random seeding 0 means we will get the same random numbers each time we run the program\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32, name='dropout_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll use the truncated_normal as it is the recomended random to use for the relu \n",
    "# activation function\n",
    "X = tf.placeholder(tf.float32, [None, 784], 'image')\n",
    "\n",
    "\n",
    "# first layer - relu activation\n",
    "W1 = tf.Variable(tf.truncated_normal([784,200], stddev=0.1), 'l1_weights')\n",
    "B1 = tf.Variable(tf.truncated_normal([200], stddev=0.1), 'l1_baiases')\n",
    "Y1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "\n",
    "# second - relu activation\n",
    "W2 = tf.Variable(tf.truncated_normal([200,100], stddev=0.1), 'l2_weights')\n",
    "B2 = tf.Variable(tf.truncated_normal([100], stddev=0.1), 'l2_baiases')\n",
    "Y2 = tf.nn.relu(tf.add(tf.matmul(Y1d, W2), B2))\n",
    "Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "\n",
    "# hird - relu activation\n",
    "W3 = tf.Variable(tf.truncated_normal([100,60], stddev=0.1), 'l3_weights')\n",
    "B3 = tf.Variable(tf.truncated_normal([60], stddev=0.1), 'l3_baiases')\n",
    "Y3 = tf.nn.relu(tf.add(tf.matmul(Y2d, W3), B3))\n",
    "Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "\n",
    "# forth - relu activation\n",
    "W4 = tf.Variable(tf.truncated_normal([60,30], stddev=0.1), 'l4_weights')\n",
    "B4 = tf.Variable(tf.truncated_normal([30], stddev=0.1), 'l4_baiases')\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\n",
    "Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "# last (output one) - softmax activation\n",
    "W5 = tf.Variable(tf.truncated_normal([30,10], stddev=0.1), 'output_weights')\n",
    "B5 = tf.Variable(tf.truncated_normal([10], stddev=0.1), 'output_baiases')\n",
    "# this was the mistake I made in the previous improvement.\n",
    "# TODO think of why I need to feed YLogits in the cost functiojn\n",
    "Ylogits = tf.matmul(Y4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "\n",
    "# we delcare the expected correct answers placeholder which will be used to provide the expected results\n",
    "# for each set of images. This will be used in the flow to calculate the error value\n",
    "Y_ = tf.placeholder(tf.float32, shape=[None, 10], name='expected_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cost = tf.reduce_mean(cost)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learning rate input\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "max_learning_rate = 0.003\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 20.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "\n",
    "# learning rate decay\n",
    "def get_learning_rate(i):\n",
    "    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the optimizer taking the learning rate input which will\n",
    "# be configured at each step and will minimize the corss entropy function\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as first version:\n",
    "\n",
    "# % of correct answers found in batch\n",
    "# based on the Y and Y_ which will be geneated during the runtime of the tests\n",
    "# we compose this is_correct to compute how right we are during training\n",
    "is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 0, 'learning rate', 0.003)\n",
      "('Current training accuracy', 0.92000002, ' current error', 27.197239)\n",
      "('Current testing accuracy', 0.93940002)\n",
      "('Epoch', 1, 'learning rate', 0.0028585653310520707)\n",
      "('Current training accuracy', 0.94999999, ' current error', 17.574772)\n",
      "('Current testing accuracy', 0.94809997)\n",
      "('Epoch', 2, 'learning rate', 0.0027240285123042826)\n",
      "('Current training accuracy', 0.98000002, ' current error', 7.4490466)\n",
      "('Current testing accuracy', 0.95840001)\n",
      "('Epoch', 3, 'learning rate', 0.0025960531316326675)\n",
      "('Current training accuracy', 0.94999999, ' current error', 15.699354)\n",
      "('Current testing accuracy', 0.96060002)\n",
      "('Epoch', 4, 'learning rate', 0.0024743191839261473)\n",
      "('Current training accuracy', 0.95999998, ' current error', 9.6945591)\n",
      "('Current testing accuracy', 0.96530002)\n",
      "('Epoch', 5, 'learning rate', 0.002358522270907074)\n",
      "('Current training accuracy', 0.98000002, ' current error', 5.834671)\n",
      "('Current testing accuracy', 0.96600002)\n",
      "('Epoch', 6, 'learning rate', 0.002248372839976982)\n",
      "('Current training accuracy', 0.95999998, ' current error', 7.3556447)\n",
      "('Current testing accuracy', 0.96780002)\n",
      "('Epoch', 7, 'learning rate', 0.002143595460184269)\n",
      "('Current training accuracy', 0.97000003, ' current error', 13.676487)\n",
      "('Current testing accuracy', 0.9691)\n",
      "('Epoch', 8, 'learning rate', 0.002043928133503354)\n",
      "('Current training accuracy', 0.95999998, ' current error', 10.198732)\n",
      "('Current testing accuracy', 0.9702)\n",
      "('Epoch', 9, 'learning rate', 0.001949121639703143)\n",
      "('Current training accuracy', 1.0, ' current error', 1.4608164)\n",
      "('Current testing accuracy', 0.97100002)\n",
      "('Epoch', 10, 'learning rate', 0.0018589389131666372)\n",
      "('Current training accuracy', 0.98000002, ' current error', 8.3220139)\n",
      "('Current testing accuracy', 0.97060001)\n",
      "('Epoch', 11, 'learning rate', 0.0017731544501034114)\n",
      "('Current training accuracy', 0.98000002, ' current error', 7.4120245)\n",
      "('Current testing accuracy', 0.9702)\n",
      "('Epoch', 12, 'learning rate', 0.0016915537446726768)\n",
      "('Current training accuracy', 0.99000001, ' current error', 2.019171)\n",
      "('Current testing accuracy', 0.97359997)\n",
      "('Epoch', 13, 'learning rate', 0.0016139327526069466)\n",
      "('Current training accuracy', 0.99000001, ' current error', 2.8013644)\n",
      "('Current testing accuracy', 0.97189999)\n",
      "('Epoch', 14, 'learning rate', 0.0015400973809950877)\n",
      "('Current training accuracy', 1.0, ' current error', 3.0467377)\n",
      "('Current testing accuracy', 0.97610003)\n",
      "('Epoch', 15, 'learning rate', 0.0014698630029489428)\n",
      "('Current training accuracy', 0.97000003, ' current error', 8.0109978)\n",
      "('Current testing accuracy', 0.9716)\n",
      "('Epoch', 16, 'learning rate', 0.0014030539959399427)\n",
      "('Current training accuracy', 1.0, ' current error', 0.45344251)\n",
      "('Current testing accuracy', 0.9745)\n",
      "('Epoch', 17, 'learning rate', 0.0013395033026513076)\n",
      "('Current training accuracy', 1.0, ' current error', 1.7787505)\n",
      "('Current testing accuracy', 0.9734)\n",
      "('Epoch', 18, 'learning rate', 0.0012790520132477375)\n",
      "('Current training accuracy', 0.98000002, ' current error', 5.041821)\n",
      "('Current testing accuracy', 0.9745)\n",
      "('Epoch', 19, 'learning rate', 0.0012215489680180538)\n",
      "('Current training accuracy', 0.97000003, ' current error', 3.952599)\n",
      "('Current testing accuracy', 0.97180003)\n",
      "('Epoch', 20, 'learning rate', 0.0011668503793971828)\n",
      "('Current training accuracy', 0.99000001, ' current error', 4.1922812)\n",
      "('Current testing accuracy', 0.97530001)\n",
      "('Epoch', 21, 'learning rate', 0.0011148194724223506)\n",
      "('Current training accuracy', 0.99000001, ' current error', 2.6968017)\n",
      "('Current testing accuracy', 0.97289997)\n",
      "('Epoch', 22, 'learning rate', 0.0010653261427244307)\n",
      "('Current training accuracy', 1.0, ' current error', 0.85012668)\n",
      "('Current testing accuracy', 0.97490001)\n",
      "('Epoch', 23, 'learning rate', 0.0010182466311992545)\n",
      "('Current training accuracy', 1.0, ' current error', 0.27251372)\n",
      "('Current testing accuracy', 0.97490001)\n",
      "('Epoch', 24, 'learning rate', 0.0009734632145453863)\n",
      "('Current training accuracy', 0.99000001, ' current error', 2.7236552)\n",
      "('Current testing accuracy', 0.97469997)\n",
      "('Epoch', 25, 'learning rate', 0.0009308639108945514)\n",
      "('Current training accuracy', 0.99000001, ' current error', 2.022536)\n",
      "('Current testing accuracy', 0.9745)\n",
      "('Epoch', 26, 'learning rate', 0.0008903421997986366)\n",
      "('Current training accuracy', 0.99000001, ' current error', 1.6455028)\n",
      "('Current testing accuracy', 0.97619998)\n",
      "('Epoch', 27, 'learning rate', 0.0008517967558730855)\n",
      "('Current training accuracy', 0.99000001, ' current error', 2.1000822)\n",
      "('Current testing accuracy', 0.97680002)\n",
      "('Epoch', 28, 'learning rate', 0.0008151311954306589)\n",
      "('Current training accuracy', 0.97000003, ' current error', 3.658864)\n",
      "('Current testing accuracy', 0.97500002)\n",
      "('Epoch', 29, 'learning rate', 0.0007802538354720133)\n",
      "('Current training accuracy', 0.99000001, ' current error', 3.2917147)\n",
      "('Current testing accuracy', 0.97490001)\n",
      "('Final training accuracy', 1.0)\n",
      "('Final testing accuracy', 0.97359997)\n"
     ]
    }
   ],
   "source": [
    "# we start our trainig session\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for _ in range(30):\n",
    "        \n",
    "        learning_rate = get_learning_rate(_)\n",
    "        \n",
    "        print('Epoch', _, 'learning rate', learning_rate)\n",
    "        \n",
    "        for i in range(int(mnist.train.num_examples/100)):\n",
    "            # load a btach of images, thanks to mnist\n",
    "            batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "            \n",
    "            train_data = {X:batch_X, Y_:batch_Y, lr: learning_rate, pkeep: 0.75}\n",
    "            \n",
    "            # train the network one time\n",
    "            sess.run(train_step, feed_dict=train_data)\n",
    "\n",
    "\n",
    "        # get the accuracy after training\n",
    "        a, c = sess.run([accuracy, cost], feed_dict=train_data)\n",
    "        print('Current training accuracy', a, ' current error', c)\n",
    "        print('Current testing accuracy', accuracy.eval({X:mnist.test.images, Y_:mnist.test.labels, lr: learning_rate, pkeep: 0.75}))\n",
    "\n",
    "    a, c = sess.run([accuracy, cost], feed_dict=train_data)\n",
    "    print('Final training accuracy', a)        \n",
    "    print('Final testing accuracy', accuracy.eval({X:mnist.test.images, Y_:mnist.test.labels, lr: learning_rate, pkeep: 0.75}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
