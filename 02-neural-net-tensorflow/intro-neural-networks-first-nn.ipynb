{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll take data from http://yann.lecun.com/exdb/mnist/ for digits\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic idea of the NN\n",
    "#\n",
    "# repeat milions of times the two steps bellow:\n",
    "#\n",
    "# step 1: feed froward\n",
    "#\n",
    "# we get input image -> send it to input layer -> weights \n",
    "#   -> hidden layer 1 -> weights \n",
    "#   -> hidden layer 2 -> weights \n",
    "#   -> output \n",
    "#\n",
    "# step 2: backpropagation\n",
    "#\n",
    "# compare output to intended outpuntt (we have indended from the data) -> cost function (corss entropy)\n",
    "# cross entropy wiki: https://en.wikipedia.org/wiki/Cross_entropy\n",
    "#\n",
    "# optimization function -> attempts to mimzie cost function by manipulating the weights\n",
    "# AdamOptimizer, SGD (gradient descnet) etc\n",
    "# \n",
    "# feed forward + backpropagation = epoc (one iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# we already have the data available in the examples file\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# one_hot = True means we represent the data in a vector way, see below\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "\n",
    "# 10 classes, from 0 to 9\n",
    "# 0 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] because one_hot=true in read_data_sets\n",
    "# 1 = [0, 1, ....] etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we declare the number of newurons for each layer to be used \n",
    "# in configuration for tensureflow\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 620\n",
    "n_nodes_hl3 = 300\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "# batch size = how many images we will send once before we backpropagate\n",
    "# how many photos we send before we change weights in the neural network\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we declare inputs for the image. In tensorflow we declare the inputs\n",
    "# as these placeholder which will be treated as input 'gates' of sorts\n",
    "# when we run the program\n",
    "\n",
    "# the height x width of the image, so the number of pixels we are sending \n",
    "x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "\n",
    "# y is what will be placing as expected values\n",
    "y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# each layer in the network will have one weight and one baias for each 'neuron'\n",
    "# each neuron will receive all the inputs. \n",
    "# \n",
    "# this means that for each input value, we will run all the wieghts and add all the baiases?\n",
    "def neural_network_layer_initialisation(input_count, output_count):\n",
    "    layer_configuration = {\n",
    "        'weigths': tf.Variable(\n",
    "            # gets a random matrix of values, with inputcount rows and outputcount columns\n",
    "            tf.random_normal([\n",
    "                input_count, output_count\n",
    "            ])\n",
    "        ),\n",
    "        'biases': tf.Variable(\n",
    "            # gets a random row of values, of output_count\n",
    "            tf.random_normal(\n",
    "                [output_count]\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "    return layer_configuration\n",
    "    \n",
    "# cool reason for biases. Because we might have 0 input, and 0 * weights = 0 all the time. \n",
    "# with baias, we might end up with neurons being triggered regardless of 0. \n",
    "# y = w * x + b (wieghts multiplied by input, adding bias afterwards) => output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data): \n",
    "    # first layer gets the image, so 784 pixel values and returns one value out of \n",
    "    # each neuron of the first layer\n",
    "    hidden_1_layer = neural_network_layer_initialisation(784, n_nodes_hl1)\n",
    "    # second layer gets the input from first layer and outputs a set of values\n",
    "    # equal with the number of neurons on the second layer\n",
    "    hidden_2_layer = neural_network_layer_initialisation(n_nodes_hl1, n_nodes_hl2)\n",
    "\n",
    "    hidden_3_layer = neural_network_layer_initialisation(n_nodes_hl2, n_nodes_hl3)\n",
    "    \n",
    "    # output get the values from the third layer and outputs one number for each n_classes\n",
    "    # we have. We will train the network and force it to return the one_hot=True representation\n",
    "    # we discussed above, so for 0, we would expect the output to be [1, 0, 0 ...]\n",
    "    output_layer = neural_network_layer_initialisation(n_nodes_hl3, n_classes)    \n",
    "    \n",
    "    \n",
    "    # we now describe in therms that tensurflow understand the way the layers \n",
    "    # propagate data, in terms of matrix computations\n",
    "    l1 = tf.add(\n",
    "        tf.matmul(data, hidden_1_layer['weigths']), hidden_1_layer['biases']\n",
    "    )\n",
    "    ## todo read about\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    \n",
    "    # layer two takes layer 1 output and does its thing\n",
    "    l2 = tf.add(\n",
    "        tf.matmul(l1, hidden_2_layer['weigths']), hidden_2_layer['biases']\n",
    "    )\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    \n",
    "    # layer two takes layer 2 output and does its thing\n",
    "    l3 = tf.add(\n",
    "        tf.matmul(l2, hidden_3_layer['weigths']), hidden_3_layer['biases']\n",
    "    )\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    print output_layer\n",
    "    # output layer takes layer 3 output and does its thing\n",
    "    output = tf.matmul(l3, output_layer['weigths']) + output_layer['biases']\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_netowrk(x, y, hm_epochs):\n",
    "    \n",
    "    # the final step of the configuration, we specify how the training session would behave\n",
    "    prediction = neural_network_model(x)\n",
    "    \n",
    "    cost = tf.reduce_mean( \n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) \n",
    "    )\n",
    "    \n",
    "    # learning_rate = 0.001 is the default learning rate for this optimizer\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    \n",
    "    # we begin the training\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for epoch in range(1, hm_epochs):\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                # it so happens that the minst data set gives us batches of images\n",
    "                # via this helper function so we can focus on JUST the AI implementation\n",
    "                e_x, e_y = mnist.train.next_batch(batch_size)\n",
    "                \n",
    "                a, c = sess.run([optimizer, cost], feed_dict = { x:e_x, y:e_y })\n",
    "                \n",
    "                epoch_loss += c\n",
    "            \n",
    "            print('Epoc', epoch, 'complete out of', hm_epochs, 'loss:', epoch_loss)\n",
    "            print('Accuracy:', accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "        \n",
    "        \n",
    "        print('Accuracy:', accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weigths': <tf.Variable 'Variable_38:0' shape=(300, 10) dtype=float32_ref>, 'biases': <tf.Variable 'Variable_39:0' shape=(10,) dtype=float32_ref>}\n",
      "('Epoc', 1, 'complete out of', 20, 'loss:', 1326739.6838684082)\n",
      "('Accuracy:', 0.90100002)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ade9975b0e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_neural_netowrk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-4ec4c42d6c6d>\u001b[0m in \u001b[0;36mtrain_neural_netowrk\u001b[0;34m(x, y, hm_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0me_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me_y\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_neural_netowrk(x, y, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
